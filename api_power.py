import io
import os
import time
import shutil
import tempfile
import asyncio
import torch
import torchaudio
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import StreamingResponse
from speechbrain.inference.separation import SepformerSeparation

# --- å…¨å±€å˜é‡ ---
model = None
device = None
gpu_lock = asyncio.Lock()  # å¿…é¡»åŠ é”ï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚å¯¼è‡´ GPU æ˜¾å­˜å†²çª

# --- 1. ç”Ÿå‘½å‘¨æœŸç®¡ç† (å¯åŠ¨åŠ è½½ & é¢„çƒ­) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global model, device
    print("â³ [Startup] æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
    
    # ç¡¬ä»¶é…ç½®
    if torch.cuda.is_available():
        device = "cuda"
        # å…³é—­ benchmark é¿å…é¦–æ¬¡åŠ¨æ€æœç´¢ç®—æ³•è€—æ—¶
        torch.backends.cudnn.benchmark = False 
        # Ampere æ¶æ„å¼€å¯ TF32
        if torch.cuda.get_device_capability()[0] >= 8:
            torch.set_float32_matmul_precision('high')
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: GPU ({torch.cuda.get_device_name(0)})")
    else:
        device = "cpu"
        print("âš ï¸ ä½¿ç”¨è®¾å¤‡: CPU")

    # åŠ è½½æ¨¡å‹
    print("â³ [Startup] æ­£åœ¨åŠ è½½æ¨¡å‹ (å¸¸é©»å†…å­˜)...")
    run_opts = {"device": device}
    model = SepformerSeparation.from_hparams(
        source="speechbrain/sepformer-wsj03mix",
        savedir="pretrained_models/sepformer-wsj03mix",
        run_opts=run_opts
    )
    model.eval()

    # é¢„çƒ­ GPU
    if device == "cuda":
        print("ğŸ”¥ [Startup] æ­£åœ¨é¢„çƒ­ GPU...")
        dummy_input = torch.randn(1, 8000).to(device)
        with torch.inference_mode():
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                _ = model.separate_batch(dummy_input)
        torch.cuda.synchronize()
        print("âœ… [Startup] é¢„çƒ­å®Œæˆï¼ŒæœåŠ¡å·²å°±ç»ªï¼")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ›‘ [Shutdown] æœåŠ¡å…³é—­ï¼Œæ¸…ç†èµ„æº...")
    if device == "cuda":
        torch.cuda.empty_cache()

# --- 2. åˆå§‹åŒ– FastAPI ---
app = FastAPI(title="Audio Separation API", lifespan=lifespan)

# --- 3. æ ¸å¿ƒæ¥å£é€»è¾‘ ---
@app.post("/separate")
async def separate_audio_endpoint(file: UploadFile = File(...)):
    """
    ä¸Šä¼ æ··åˆéŸ³é¢‘ï¼Œè¿”å›åˆ†ç¦»åèƒ½é‡æœ€å¤§çš„éŸ³é¢‘æµ (WAVæ ¼å¼)
    """
    global model, device

    # æ­¥éª¤ A: ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    # SpeechBrain éœ€è¦æ–‡ä»¶è·¯å¾„ä½œä¸ºè¾“å…¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_input:
        shutil.copyfileobj(file.file, temp_input)
        temp_input_path = temp_input.name

    try:
        # æ­¥éª¤ B: è·å– GPU é”å¹¶æ‰§è¡Œæ¨ç†
        # ä½¿ç”¨ async with gpu_lock ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨ä½¿ç”¨ GPU
        async with gpu_lock:
            start_time = time.time()
            
            # --- æé€Ÿæ¨ç†æ ¸å¿ƒ ---
            with torch.inference_mode():
                if device == "cuda":
                    with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                        est_sources = model.separate_file(path=temp_input_path)
                else:
                    est_sources = model.separate_file(path=temp_input_path)
                
                # --- èƒ½é‡ç­›é€‰ (GPUå†…å®Œæˆ) ---
                # est_sources: [batch=1, time, sources]
                # è®¡ç®—å¹³æ–¹å’Œèƒ½é‡ï¼Œæ‰¾å‡ºæœ€å¤§å€¼çš„ç´¢å¼•
                energies = est_sources.pow(2).sum(dim=1).squeeze()
                best_idx = torch.argmax(energies).item()
                best_source = est_sources[:, :, best_idx]
            
            if device == "cuda":
                torch.cuda.synchronize()
            
            infer_time = time.time() - start_time
            print(f"âœ… [Request] æ¨ç†å®Œæˆï¼Œè€—æ—¶: {infer_time:.4f}s | é€‰ä¸­æºç´¢å¼•: {best_idx}")

        # æ­¥éª¤ C: å°†ç»“æœå†™å…¥å†…å­˜ Buffer (ä¸å†™ç£ç›˜ï¼Œé€Ÿåº¦æ›´å¿«)
        # å¿…é¡»è½¬å› float32 å¦åˆ™ wav ç¼–ç ä¼šæŠ¥é”™
        source_cpu = best_source.detach().cpu().float()
        
        buffer = io.BytesIO()
        torchaudio.save(buffer, source_cpu, 8000, format="wav")
        buffer.seek(0) # æŒ‡é’ˆå›åˆ°å¼€å¤´

        # æ­¥éª¤ D: è¿”å›æµå¼å“åº”
        return StreamingResponse(
            buffer, 
            media_type="audio/wav",
            headers={"Content-Disposition": f"attachment; filename=best_source_{best_idx}.wav"}
        )

    except Exception as e:
        return {"error": str(e)}
        
    finally:
        # æ­¥éª¤ E: æ¸…ç†ä¸´æ—¶è¾“å…¥æ–‡ä»¶
        if os.path.exists(temp_input_path):
            os.remove(temp_input_path)

if __name__ == "__main__":
    import uvicorn
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(app, host="0.0.0.0", port=8000)
